---
title: "Practical Machine Learning - Assignment"
author: "Juan A. Garcia"
date: "19 January 2016"
output: html_document
---


# Summary

The goal of the project is to predict how users did exercise,from the data adquired using accelerometers on the belt,arm, and dumbell by 6 participants.
After a data clean up, we have selected the model with best accuracy, and used that model to predict the 20 cases from the test data file.

The models tested were Random Forest, boosting with trees, Linear Discriminant analysis (lda), decissions trees, and Support Vector Machine. I also made a combined model usign the 2 models with best accuracy, which were *Random Forest* and *Support Vector Machine*.   

The one selected was the combinations of those 2 with better accuracy, as it had the best accuracy (**0.9838907**). The 20 cases from the test data set will be used for the grading test. 


# Data clean up

The first thing is to load both data sets and split the training data, making sure to read "NA", and "#DIV/0!" as NaN in R.

```{r}
set.seed(6699)
training <- read.csv("pml-training.csv", na.strings=c("NA","#DIV/0!","") )
testing  <- read.csv("pml-testing.csv",  na.strings=c("NA","#DIV/0!","") )
dim(training)
``` 

From a summary of the data, we find out that some variable have a huge numebr of NA in the value, so we remove those predictors with a high number of NA.
And we will also remove from the dataset those variables which are not data from the sensor, like name or time related data:

```{r}
cols.to.select <- colSums(is.na(training))==0 #find columns with high number of NA values
cols.to.select <- cols.to.select & !(names(training) %in% c('X', 'user_name', 'new_window', 'num_window', 'raw_timestamp_part_1', 'raw_timestamp_part_2', 'cvtd_timestamp'))
cols.to.select.t <- colSums(is.na(testing))==0 
cols.to.select.t <- cols.to.select.t & !(names(testing) %in% c('X', 'user_name', 'new_window', 'num_window', 'raw_timestamp_part_1', 'raw_timestamp_part_2', 'cvtd_timestamp')) 
training <- training[, cols.to.select] #remove columns not to use
testing <- testing[, cols.to.select.t]
```

We have reduce the dimension (number of predictor variables) down to :
```{r}
dim(training)
```
 
### Sample data
Now, to be able to build and valdiate the model, we need to divide the training data into 2 sets that will be used to train and to test the training. This way we will find out which model fits best.

```{r, echo=FALSE }
library(caret);
library(randomForest);
library(e1071);
```

```{r}
inTrain<-createDataPartition (y=training$classe, p=0.75, list=FALSE)
training.in<-training[inTrain,]
testing.in<-training[-inTrain,]
```

```{r, echo=FALSE}
### Basic preprocessing

#training.in<-preProcess(training.in[,-1], method=c('center','scale') )
#testing.in<-preProcess(testing.in[,-1], method=c('center','scale') )
```

### PCA analysis
To make the model simpler, reducing the number of predictor but also the noice due to avering, we will preprocess teh data uning Principal Component Analyis (PCA)
```{r}
preproc <- preProcess(training.in[,-1], method='pca', thresh=0.99) #use the PCA preprocess
training.pca <- predict(preproc, training.in[,-1])    #apply to all vars except last one (classe) 
testing.pca <- predict(preproc, testing.in[,-1])
testing_to_predict.pca <- predict(preproc, testing[,-1])
dim(training.pca)
```
We have simplified further the model to down to 37 principal variables. Now that the data is cleaned, let's train the different models

# Training the model

We will use different models and select the most accuracy one. We will use: *Random Forest*, *boosting with trees*, *Linear Discriminant analysis (lda)*, *decissions trees*, and *Support Vector Machine* 

```{r, results="hide"}
fit.rf   <- train(training.in$classe  ~ .,method='rf'   ,data=training.pca)
fit.gbm  <- train(training.in$classe  ~ .,method="gbm"  ,data=training.pca)
fit.svm  <- svm(training.in$classe  ~ . ,data=training.pca)
fit.lda  <- train(training.in$classe  ~ .,method="lda"  ,data=training.pca)
fit.rpart<- train(training.in$classe  ~ .,method="rpart",data=training.pca)
```

Once we have the model trainied, we will use the testing samples to predict.
# Validation
```{r}
pred.rf    <- predict(fit.rf,testing.pca)
pred.gbm   <- predict(fit.gbm,testing.pca)
pred.svm   <- predict(fit.svm,testing.pca) 
pred.lda   <- predict(fit.lda,testing.pca)
pred.rpart <- predict(fit.rpart,testing.pca) 
```

For each predict, we will calculate the confussion matrix, and we get the accuracy from it, to select which model is the one that we will select for the 20 entries to predict.

# Evaluation

```{r}
confusionMatrix(pred.rf, testing.pca$classe)$overall["Accuracy"]
confusionMatrix(pred.gbm, testing.pca$classe)$overall["Accuracy"]
confusionMatrix(pred.svm, testing.pca$classe)$overall["Accuracy"]
confusionMatrix(pred.lda, testing.pca$classe)$overall["Accuracy"]
confusionMatrix(pred.rpart, testing.pca$classe)$overall["Accuracy"]
```

Given those accuracies, the 2 best models are the one from the Ramdom Forest and Support Vector Machine.

Let's combine both to see if combined they can increase a bit the accuracy.
```{r}
pred.dataframe <- data.frame(pred.rf,pred.svm, classe=testing.pca$classe)
fit.combined <- train(classe ~.,data=pred.dataframe)
pred.combined <- predict(fit.combined,pred.dataframe)
#Let's get the accuracy of the combined model
confusionMatrix(pred.combined, testing.pca$classe)$overall["Accuracy"]
```

From them confussion matrix on the combined model, we see it increases a bit the accuracy to *0.9838907* , so this will be the model we will finally chose for predicting with the test data set read from the test file. The increase is very minor, but we'll use it.

```{r}
 pred.1<-predict(fit.rf,testing_to_predict.pca)
 pred.2<-predict(fit.svm,testing_to_predict.pca)
 combined.to.predict<-data.frame(pred.rf=pred.1, pred.svm=pred.2)
 results<-predict(fit.combined, combined.to.predict)
 results
```



